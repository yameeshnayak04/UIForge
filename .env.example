# LLM Provider Configuration
# Options: mock | openai | gemini
LLM_PROVIDER=gemini

# OpenAI Configuration (if using OpenAI)
OPENAI_API_KEY=sk-your-openai-key-here
OPENAI_MODEL=gpt-4o-mini

# Google Gemini Configuration (if using Gemini)
GEMINI_API_KEY=your-gemini-api-key-here
GEMINI_MODEL=gemini-2.5-flash

# Optional: Use LLM for explanation step (slower). Default is deterministic local explainer.
LLM_EXPLAINER=false

# Optional: automatic fallback when primary provider hits quota/rate limits.
# Options: mock | openai | gemini
LLM_FALLBACK_PROVIDER=mock
